
# ACME: Adaptive Customization of Large Models via Distributed Systems

[![åˆ‡æ¢ä¸ºè‹±æ–‡ç‰ˆ](https://img.shields.io/badge/GitHub-Chinese-blue?logo=github)](./README_CN.md)

This repository contains the official implementation of the ICDCS 2025 accepted paper **"ACME: Adaptive Customization of Large Models via Distributed Systems"**.

## ðŸ“šOverview

ACME (Adaptive Customization of Large Models via Distributed Systems) is a framework designed to adaptively customize large Transformer-based models across distributed systems. It addresses major deployment challenges such as **model-device mismatch, resource constraints, and data heterogeneity**. ACME proposes a **bidirectional single-loop distributed system** that enables progressive model customization through collaboration between the cloud, edge servers, and devices. By separating **backbone generation** and **data-aware header refinement**, and avoiding direct transmission of local data, ACME achieves **high accuracy with minimal communication cost**, enabling efficient and personalized model deployment at scale.

## ðŸ“ Project Structure

```
â”œâ”€â”€ data/                    # Datasets (place CIFAR-100 or Stanford Cars here)
â”œâ”€â”€ draw/                    # Figure generation scripts and outputs
â”‚		â”œâ”€â”€ draw_challenge_motiv/            
â”‚   â””â”€â”€ draw_experi/  			 
â”œâ”€â”€ log/                     # Logs during training/experiments
â”œâ”€â”€ model/                   # Model definitions and pretrained models
â”œâ”€â”€ moti/                    # Motivation analysis materials
â”‚   â””â”€â”€ origin_motiv/
â”‚       â””â”€â”€ log/             # Original motivation experiment logs
â”œâ”€â”€ runs/                    # Experimental results and intermediate files
â””â”€â”€ src/                     # Main source code
    â”œâ”€â”€ backup/              # Customized files to override parts of Transformers
    â””â”€â”€ shell/               # Shell scripts for automated execution
```

## ðŸ› ï¸ Prerequisites

1. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

2. Download the pretrained model:

   - [vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)
   - Place it in the `model/` directory.

3. Download datasets:

   - [CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html)
   - [Stanford Cars](https://ai.stanford.edu/~jkrause/cars/car_dataset.html)
   - Place them in the `data/` directory.

4. âš ï¸ **Important: Overwriting Transformers Source Code**

   To enable customized behavior, you must **manually replace** parts of the Transformers library:

   - Locate the installation path of your local Transformers library (typically `site-packages/transformers/`)
   - Copy and **overwrite** the corresponding files from `src/backup/` into the above directory

   > âš  Warning: This operation modifies the default implementation of Transformers. Proceed only if you understand the changes, and it is highly recommended to use a virtual environment to avoid affecting other projects.


## ðŸš€ How to Run

1. Run cloud pretraining:

   ```bash
   python cloud_pretrain.py
   ```

2. Run cloud script for **DynaViTw**:

   ```bash
   bash ./src/shell/run_cloud1.sh
   ```

3. Run cloud script for **DynaViT**:

   ```bash
   bash ./src/shell/run_cloud2.sh
   ```

4. Run edge-side NAS:

   ```bash
   bash ./src/shell/run_nas.sh
   ```

## ðŸ“¦ Model and Dataset Links

| Type | Name | Link |
|------|------|------|
| Model | vit-base-patch16-224 | [Download from Hugging Face](https://huggingface.co/google/vit-base-patch16-224) |
| Dataset | CIFAR-100 | [Official CIFAR Site](https://www.cs.toronto.edu/~kriz/cifar.html) |
| Dataset | Stanford Cars | [Stanford AI Lab](https://ai.stanford.edu/~jkrause/cars/car_dataset.html) |
